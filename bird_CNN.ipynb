{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "354ac32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "# 1. Load data\n",
    "data_folder = \"Pre-Processed Dark-Eyed Junco Data/\"\n",
    "env = pd.read_csv(data_folder + \"environmental_vars_checklists_md_jan.csv\")\n",
    "checklists = pd.read_csv(data_folder + \"checklists_zf_md_deju_jan.csv\")\n",
    "train_df = pd.merge(checklists, env, on=\"checklist_id\")\n",
    "\n",
    "features = ['year', 'day_of_year', 'hours_of_day',\n",
    "            'effort_hours', 'effort_distance_km', 'effort_speed_kmph',\n",
    "            'number_observers'] + \\\n",
    "           [col for col in train_df.columns if col.startswith(('pland_', 'ed_', 'elevation_'))]\n",
    "\n",
    "X = train_df[features]\n",
    "y = train_df['species_observed'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf81ed",
   "metadata": {},
   "source": [
    "## Dense NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f3060d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1227.0027\n",
      "Best threshold: 0.475, MCC: 0.343, F1: 0.593\n",
      "Epoch 2, Loss: 1176.8795\n",
      "Best threshold: 0.465, MCC: 0.367, F1: 0.606\n",
      "Epoch 3, Loss: 1151.3612\n",
      "Best threshold: 0.485, MCC: 0.392, F1: 0.629\n",
      "Epoch 4, Loss: 1130.3626\n",
      "Best threshold: 0.465, MCC: 0.409, F1: 0.636\n",
      "Epoch 5, Loss: 1116.3775\n",
      "Best threshold: 0.384, MCC: 0.410, F1: 0.662\n",
      "Epoch 6, Loss: 1100.2275\n",
      "Best threshold: 0.465, MCC: 0.425, F1: 0.648\n",
      "Epoch 7, Loss: 1088.2512\n",
      "Best threshold: 0.465, MCC: 0.427, F1: 0.656\n",
      "Epoch 8, Loss: 1076.9627\n",
      "Best threshold: 0.505, MCC: 0.433, F1: 0.642\n",
      "Epoch 9, Loss: 1068.0152\n",
      "Best threshold: 0.465, MCC: 0.438, F1: 0.648\n",
      "Epoch 10, Loss: 1057.3877\n",
      "Best threshold: 0.495, MCC: 0.438, F1: 0.644\n",
      "Epoch 11, Loss: 1049.1332\n",
      "Best threshold: 0.475, MCC: 0.426, F1: 0.648\n",
      "Epoch 12, Loss: 1042.7897\n",
      "Best threshold: 0.444, MCC: 0.436, F1: 0.654\n",
      "Epoch 13, Loss: 1034.0600\n",
      "Best threshold: 0.434, MCC: 0.438, F1: 0.666\n",
      "Epoch 14, Loss: 1028.4472\n",
      "Best threshold: 0.444, MCC: 0.440, F1: 0.664\n",
      "Epoch 15, Loss: 1020.4729\n",
      "Best threshold: 0.475, MCC: 0.440, F1: 0.651\n",
      "Epoch 16, Loss: 1017.5163\n",
      "Best threshold: 0.404, MCC: 0.442, F1: 0.672\n",
      "Epoch 17, Loss: 1010.1108\n",
      "Best threshold: 0.424, MCC: 0.451, F1: 0.670\n",
      "Epoch 18, Loss: 1005.7311\n",
      "Best threshold: 0.475, MCC: 0.451, F1: 0.665\n",
      "Epoch 19, Loss: 999.4264\n",
      "Best threshold: 0.465, MCC: 0.437, F1: 0.657\n",
      "Epoch 20, Loss: 995.3074\n",
      "Best threshold: 0.455, MCC: 0.453, F1: 0.667\n"
     ]
    }
   ],
   "source": [
    "# 2. Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=28)\n",
    "\n",
    "# 3. Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# 4. Define the model\n",
    "class SpeciesNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = SpeciesNet(X_train.shape[1]).to(device)\n",
    "criterion = nn.BCELoss().to(device)\n",
    "accuracy = BinaryAccuracy().to(device)\n",
    "f1 = BinaryF1Score().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 5. Training loop\n",
    "train_accuracy = []\n",
    "train_f1 = []\n",
    "val_accuracy = []\n",
    "val_f1 = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        accuracy.update(preds.squeeze(1), yb.squeeze(1))\n",
    "        f1.update(preds.squeeze(1), yb.squeeze(1))\n",
    "    train_accuracy.append(accuracy.compute().cpu().numpy())\n",
    "    train_f1.append(f1.compute().cpu().numpy())\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    accuracy.reset()\n",
    "    f1.reset()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_probs = model(X_train_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    cal = IsotonicRegression(out_of_bounds='clip')\n",
    "    cal.fit(train_probs, y_train)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            val_probs_raw = model(X_val_tensor)\n",
    "    #         accuracy.update(val_probs_raw.squeeze(1), yb.squeeze(1))\n",
    "    #         f1.update(val_probs_raw.squeeze(1), yb.squeeze(1))\n",
    "    # val_accuracy.append(accuracy.compute().cpu().numpy())\n",
    "    # val_f1.append(f1.compute().cpu().numpy())\n",
    "\n",
    "    val_probs_cal = cal.predict(val_probs_raw.cpu().numpy().flatten())\n",
    "\n",
    "    # 7. Threshold tuning\n",
    "    best_mcc, best_f1, best_thresh = -1, -1, 0\n",
    "    for t in np.linspace(0, 1, 100):\n",
    "        preds = (val_probs_cal > t).astype(int)\n",
    "        m = matthews_corrcoef(y_val, preds)\n",
    "        f = f1_score(y_val, preds)\n",
    "        if m > best_mcc:\n",
    "            best_mcc, best_f1, best_thresh = m, f, t\n",
    "\n",
    "    print(f\"Best threshold: {best_thresh:.3f}, MCC: {best_mcc:.3f}, F1: {best_f1:.3f}\")\n",
    "    accuracy.reset()\n",
    "    f1.reset()\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2300d740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.455, MCC: 0.453, F1: 0.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alonz\\AppData\\Local\\Temp\\ipykernel_19764\\530711701.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  grid_output[\"in_range\"] = (grid_output[\"encounter_rate\"] > best_thresh).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# 6. Predict and calibrate with isotonic regression\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_probs = model(X_train_tensor).cpu().numpy().flatten()\n",
    "\n",
    "cal = IsotonicRegression(out_of_bounds='clip')\n",
    "cal.fit(train_probs, y_train)\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_probs_raw = model(X_val_tensor).cpu().numpy().flatten()\n",
    "val_probs_cal = cal.predict(val_probs_raw)\n",
    "\n",
    "# 7. Threshold tuning\n",
    "best_mcc, best_f1, best_thresh = -1, -1, 0\n",
    "for t in np.linspace(0, 1, 100):\n",
    "    preds = (val_probs_cal > t).astype(int)\n",
    "    m = matthews_corrcoef(y_val, preds)\n",
    "    f = f1_score(y_val, preds)\n",
    "    if m > best_mcc:\n",
    "        best_mcc, best_f1, best_thresh = m, f, t\n",
    "\n",
    "print(f\"Best threshold: {best_thresh:.3f}, MCC: {best_mcc:.3f}, F1: {best_f1:.3f}\")\n",
    "\n",
    "## Generate Confusion matrix using tuned threshold and calibrated probabilities\n",
    "best_pred = (val_probs_cal > best_thresh).astype(int)\n",
    "confmat = confusion_matrix(y_val, best_pred, normalize = 'pred')\n",
    "\n",
    "# 8. Predict on grid\n",
    "grid = pd.read_csv(data_folder + \"environmental_vars_prediction_grid_md.csv\")\n",
    "grid[\"observation_date\"] = pd.to_datetime(\"2023-01-15\")\n",
    "grid[\"year\"] = grid[\"observation_date\"].dt.year\n",
    "grid[\"day_of_year\"] = grid[\"observation_date\"].dt.dayofyear\n",
    "grid[\"hours_of_day\"] = 7.5\n",
    "grid[\"effort_distance_km\"] = 2\n",
    "grid[\"effort_hours\"] = 1\n",
    "grid[\"effort_speed_kmph\"] = 2\n",
    "grid[\"number_observers\"] = 1\n",
    "\n",
    "X_grid = grid[features]\n",
    "X_grid_scaled = scaler.transform(X_grid)\n",
    "X_grid_tensor = torch.tensor(X_grid_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    grid_probs_raw = model(X_grid_tensor).cpu().numpy().flatten()\n",
    "grid_probs_cal = cal.predict(grid_probs_raw)\n",
    "grid[\"encounter_rate\"] = np.clip(grid_probs_cal, 0, 1)\n",
    "\n",
    "# Save outputs\n",
    "grid_output = grid[[\"cell_id\", \"x\", \"y\", \"encounter_rate\"]]\n",
    "grid_output[\"in_range\"] = (grid_output[\"encounter_rate\"] > best_thresh).astype(int)\n",
    "grid_output.to_csv(\"junco_nn_predictions.csv\", index=False)\n",
    "\n",
    "# Save validation predictions for R\n",
    "results_df = pd.DataFrame({\n",
    "    'obs': y_val,\n",
    "    'pred': val_probs_cal\n",
    "})\n",
    "results_df.to_csv(\"dnn_predictions_for_r.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cde5b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Grid Predictions')\n",
    "plt.scatter(grid_output['x'], grid_output['y'], c = grid_output['encounter_rate'].values, s = 10, marker = 's', alpha = 0.8)\n",
    "plt.colorbar()\n",
    "# plt.show()\n",
    "plt.savefig('dnn_grid_pred.png')\n",
    "plt.close()\n",
    "sns.heatmap(confmat, annot=True, cmap='Greens')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "plt.savefig('dnn_conf_mat.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2332d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot accuracy and f1\n",
    "plt.plot(range(num_epochs), train_accuracy)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "# plt.show()\n",
    "plt.savefig('dnn_train_accuracy.png')\n",
    "plt.close()\n",
    "plt.plot(range(num_epochs), train_f1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Training F1 Score')\n",
    "# plt.show()\n",
    "plt.savefig('dnn_train_f1.png')\n",
    "plt.close()\n",
    "# plt.plot(range(num_epochs), val_accuracy)\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Validation Accuracy')\n",
    "# plt.savefig('dnn_val_accuracy.png')\n",
    "# plt.close()\n",
    "# plt.plot(range(num_epochs), val_f1)\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('F1 Score')\n",
    "# plt.title('Validation F1 Score')\n",
    "# plt.savefig('dnn_val_f1.png')\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81155ad",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d9b83e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1236.4819\n",
      "Best threshold: 0.434, MCC: 0.364, F1: 0.617\n",
      "Epoch 2, Loss: 1156.6993\n",
      "Best threshold: 0.414, MCC: 0.395, F1: 0.650\n",
      "Epoch 3, Loss: 1124.7060\n",
      "Best threshold: 0.333, MCC: 0.415, F1: 0.672\n",
      "Epoch 4, Loss: 1099.0419\n",
      "Best threshold: 0.394, MCC: 0.416, F1: 0.666\n",
      "Epoch 5, Loss: 1077.5622\n",
      "Best threshold: 0.444, MCC: 0.438, F1: 0.669\n",
      "Epoch 6, Loss: 1060.4956\n",
      "Best threshold: 0.434, MCC: 0.440, F1: 0.668\n",
      "Epoch 7, Loss: 1044.6811\n",
      "Best threshold: 0.384, MCC: 0.450, F1: 0.684\n",
      "Epoch 8, Loss: 1030.4958\n",
      "Best threshold: 0.404, MCC: 0.446, F1: 0.681\n",
      "Epoch 9, Loss: 1019.5672\n",
      "Best threshold: 0.475, MCC: 0.445, F1: 0.659\n",
      "Epoch 10, Loss: 1006.9779\n",
      "Best threshold: 0.485, MCC: 0.460, F1: 0.665\n",
      "Epoch 11, Loss: 997.7647\n",
      "Best threshold: 0.404, MCC: 0.455, F1: 0.682\n",
      "Epoch 12, Loss: 988.3809\n",
      "Best threshold: 0.485, MCC: 0.458, F1: 0.667\n",
      "Epoch 13, Loss: 978.3084\n",
      "Best threshold: 0.444, MCC: 0.442, F1: 0.666\n",
      "Epoch 14, Loss: 968.9541\n",
      "Best threshold: 0.465, MCC: 0.458, F1: 0.671\n",
      "Epoch 15, Loss: 960.9449\n",
      "Best threshold: 0.455, MCC: 0.454, F1: 0.668\n",
      "Epoch 16, Loss: 953.2192\n",
      "Best threshold: 0.434, MCC: 0.458, F1: 0.676\n",
      "Epoch 17, Loss: 944.4450\n",
      "Best threshold: 0.455, MCC: 0.463, F1: 0.677\n",
      "Epoch 18, Loss: 935.7099\n",
      "Best threshold: 0.444, MCC: 0.462, F1: 0.674\n",
      "Epoch 19, Loss: 927.7325\n",
      "Best threshold: 0.515, MCC: 0.466, F1: 0.663\n",
      "Epoch 20, Loss: 917.2828\n",
      "Best threshold: 0.424, MCC: 0.460, F1: 0.680\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class SpeciesNet1DCNN(nn.Module):\n",
    "    def __init__(self, input_length: int):\n",
    "        super().__init__()\n",
    "        # 1) 1D-CNN feature extractor\n",
    "        self.cnn = nn.Sequential(\n",
    "            # in_channels=1, out_channels=16, length stays 39 (padding=1)\n",
    "            nn.Conv1d(1, 16, kernel_size=3, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),            # -> length = floor(39/2) = 19\n",
    "\n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),            # -> length = floor(19/2) = 9\n",
    "        )\n",
    "        # compute flattened size after two pools\n",
    "        conv_out_len = input_length // 2 // 2    # 39→19→9\n",
    "        flattened_dim = 32 * conv_out_len        # 32 channels × length 9 = 288\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flattened_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, 1, 39)\n",
    "        x = self.cnn(x)                 # -> (batch, 32, 9)\n",
    "        x = x.view(x.size(0), -1)       # -> (batch, 32*9)\n",
    "        return self.fc(x)               # -> (batch, 1)\n",
    "\n",
    "# ——— data prep ———\n",
    "# 2. Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=28)\n",
    "# add a channel dimension of size 1:\n",
    "X_train_1d = X_train.reshape(-1, 1, 39)  \n",
    "X_val_1d   = X_val.reshape(  -1, 1, 39)\n",
    "\n",
    "# 3. Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_1d, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val_1d, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_ds   = TensorDataset(X_val_tensor,   y_val_tensor)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32)\n",
    "\n",
    "# ——— model, loss, optimizer ———\n",
    "model = SpeciesNet1DCNN(input_length=39).to(device)\n",
    "criterion = nn.BCELoss().to(device)\n",
    "accuracy = BinaryAccuracy().to(device)\n",
    "f1 = BinaryF1Score().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 5. Training loop\n",
    "train_accuracy = []\n",
    "train_f1 = []\n",
    "val_accuracy = []\n",
    "val_f1 = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        accuracy.update(preds.squeeze(1), yb.squeeze(1))\n",
    "        f1.update(preds.squeeze(1), yb.squeeze(1))\n",
    "    train_accuracy.append(accuracy.compute().cpu().numpy())\n",
    "    train_f1.append(f1.compute().cpu().numpy())\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    accuracy.reset()\n",
    "    f1.reset()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_probs = model(X_train_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    cal = IsotonicRegression(out_of_bounds='clip')\n",
    "    cal.fit(train_probs, y_train)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            val_probs_raw = model(X_val_tensor)\n",
    "    #         accuracy.update(val_probs_raw.squeeze(1), yb.squeeze(1))\n",
    "    #         f1.update(val_probs_raw.squeeze(1), yb.squeeze(1))\n",
    "    # val_accuracy.append(accuracy.compute().cpu().numpy())\n",
    "    # val_f1.append(f1.compute().cpu().numpy())\n",
    "\n",
    "    val_probs_cal = cal.predict(val_probs_raw.cpu().numpy().flatten())\n",
    "\n",
    "    # 7. Threshold tuning\n",
    "    best_mcc, best_f1, best_thresh = -1, -1, 0\n",
    "    for t in np.linspace(0, 1, 100):\n",
    "        preds = (val_probs_cal > t).astype(int)\n",
    "        m = matthews_corrcoef(y_val, preds)\n",
    "        f = f1_score(y_val, preds)\n",
    "        if m > best_mcc:\n",
    "            best_mcc, best_f1, best_thresh = m, f, t\n",
    "\n",
    "    print(f\"Best threshold: {best_thresh:.3f}, MCC: {best_mcc:.3f}, F1: {best_f1:.3f}\")\n",
    "    accuracy.reset()\n",
    "    f1.reset()\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d035750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.430, MCC: 0.460, F1: 0.680\n"
     ]
    }
   ],
   "source": [
    "# 6. Predict and calibrate with isotonic regression\n",
    "model.eval()\n",
    "\n",
    "# ——— get train probs ———\n",
    "with torch.no_grad():\n",
    "    train_probs = model(X_train_tensor).cpu().numpy().flatten()\n",
    "\n",
    "cal = IsotonicRegression(out_of_bounds='clip')\n",
    "cal.fit(train_probs, y_train)\n",
    "\n",
    "# ——— get val probs ———\n",
    "with torch.no_grad():\n",
    "    val_probs_raw = model(X_val_tensor).cpu().numpy().flatten()\n",
    "val_probs_cal = cal.predict(val_probs_raw)\n",
    "\n",
    "# 7. Threshold tuning on validation set\n",
    "best_mcc, best_f1, best_thresh = -1, -1, 0.0\n",
    "for t in np.linspace(0, 1, 101):\n",
    "    preds_t = (val_probs_cal > t).astype(int)\n",
    "    m = matthews_corrcoef(y_val, preds_t)\n",
    "    f = f1_score(y_val, preds_t)\n",
    "    if m > best_mcc:\n",
    "        best_mcc, best_f1, best_thresh = m, f, t\n",
    "\n",
    "print(f\"Best threshold: {best_thresh:.3f}, MCC: {best_mcc:.3f}, F1: {best_f1:.3f}\")\n",
    "\n",
    "## Generate Confusion matrix using tuned threshold and calibrated probabilities\n",
    "best_pred = (val_probs_cal > best_thresh).astype(int)\n",
    "confmat = confusion_matrix(y_val, best_pred, normalize = 'pred')\n",
    "\n",
    "# 8. Predict on grid\n",
    "grid = pd.read_csv(data_folder + \"environmental_vars_prediction_grid_md.csv\")\n",
    "grid[\"observation_date\"] = pd.to_datetime(\"2023-01-15\")\n",
    "grid[\"year\"]           = grid[\"observation_date\"].dt.year\n",
    "grid[\"day_of_year\"]    = grid[\"observation_date\"].dt.dayofyear\n",
    "grid[\"hours_of_day\"]   = 7.5\n",
    "grid[\"effort_distance_km\"] = 2\n",
    "grid[\"effort_hours\"]       = 1\n",
    "grid[\"effort_speed_kmph\"]  = 2\n",
    "grid[\"number_observers\"]   = 1\n",
    "\n",
    "# select & scale features\n",
    "X_grid = grid[features]\n",
    "X_grid_scaled = scaler.transform(X_grid)\n",
    "\n",
    "# reshape for 1D-CNN: (N, 39) -> (N, 1, 39)\n",
    "X_grid_1d = X_grid_scaled.reshape(-1, 1, X_grid_scaled.shape[1])\n",
    "X_grid_tensor = torch.tensor(X_grid_1d, dtype=torch.float32).to(device)\n",
    "\n",
    "# if your grid is very large, you could also batch it:\n",
    "# grid_ds    = TensorDataset(X_grid_tensor)\n",
    "# grid_loader= DataLoader(grid_ds, batch_size=1024, shuffle=False)\n",
    "# grid_probs_raw = []\n",
    "# with torch.no_grad():\n",
    "#     for (xb,) in grid_loader:\n",
    "#         grid_probs_raw.append(model(xb).cpu().numpy().flatten())\n",
    "# grid_probs_raw = np.concatenate(grid_probs_raw)\n",
    "\n",
    "# otherwise, just do it in one go:\n",
    "with torch.no_grad():\n",
    "    grid_probs_raw = model(X_grid_tensor).cpu().numpy().flatten()\n",
    "\n",
    "# calibrate & clip\n",
    "grid_probs_cal = cal.predict(grid_probs_raw)\n",
    "grid[\"encounter_rate\"] = np.clip(grid_probs_cal, 0, 1)\n",
    "\n",
    "# assemble & save\n",
    "grid_output = grid[[\"cell_id\", \"x\", \"y\", \"encounter_rate\"]].copy()\n",
    "grid_output[\"in_range\"] = (grid_output[\"encounter_rate\"] > best_thresh).astype(int)\n",
    "grid_output.to_csv(\"junco_cnn_predictions.csv\", index=False)\n",
    "\n",
    "# also save validation predictions for R\n",
    "results_df = pd.DataFrame({\n",
    "    'obs':  y_val,\n",
    "    'pred': val_probs_cal\n",
    "})\n",
    "results_df.to_csv(\"cnn_predictions_for_r.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "87ee7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Grid Predictions')\n",
    "plt.scatter(grid_output['x'], grid_output['y'], c = grid_output['encounter_rate'].values, s = 10, marker = 's', alpha = 0.8)\n",
    "plt.colorbar()\n",
    "# plt.show()\n",
    "plt.savefig('cnn_grid_pred.png')\n",
    "plt.close()\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(confmat, annot=True, cmap='Greens')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "plt.savefig('cnn_conf_mat.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a3936394",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot accuracy and f1\n",
    "plt.plot(range(num_epochs), train_accuracy)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "# plt.show()\n",
    "plt.savefig('cnn_train_accuracy.png')\n",
    "plt.close()\n",
    "plt.plot(range(num_epochs), train_f1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Training F1 Score')\n",
    "# plt.show()\n",
    "plt.savefig('cnn_train_f1.png')\n",
    "plt.close()\n",
    "# plt.plot(range(num_epochs), val_accuracy)\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Validation Accuracy')\n",
    "# plt.savefig('cnn_val_accuracy.png')\n",
    "# plt.close()\n",
    "# plt.plot(range(num_epochs), val_f1)\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('F1 Score')\n",
    "# plt.title('Validation F1 Score')\n",
    "# plt.savefig('cnn_val_f1.png')\n",
    "# plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
