{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "354ac32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torcheval.metrics import BinaryAccuracy, BinaryF1Score, BinaryConfusionMatrix, BinaryAUROC, BinaryAUPRC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "# 1. Load data\n",
    "data_folder = \"Pre-Processed Dark-Eyed Junco Data/\"\n",
    "env = pd.read_csv(data_folder + \"environmental_vars_checklists_md_jan.csv\")\n",
    "checklists = pd.read_csv(data_folder + \"checklists_zf_md_deju_jan.csv\")\n",
    "train_df = pd.merge(checklists, env, on=\"checklist_id\")\n",
    "\n",
    "features = ['year', 'day_of_year', 'hours_of_day',\n",
    "            'effort_hours', 'effort_distance_km', 'effort_speed_kmph',\n",
    "            'number_observers'] + \\\n",
    "           [col for col in train_df.columns if col.startswith(('pland_', 'ed_', 'elevation_'))]\n",
    "\n",
    "X = train_df[features]\n",
    "y = train_df['species_observed'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf81ed",
   "metadata": {},
   "source": [
    "## Dense NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f3060d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1224.4696\n",
      "Best threshold: 0.424, MCC: 0.354, F1: 0.613\n",
      "Epoch 2, Loss: 1173.5691\n",
      "Best threshold: 0.414, MCC: 0.380, F1: 0.633\n",
      "Epoch 3, Loss: 1148.6202\n",
      "Best threshold: 0.424, MCC: 0.392, F1: 0.642\n",
      "Epoch 4, Loss: 1129.8793\n",
      "Best threshold: 0.424, MCC: 0.400, F1: 0.636\n",
      "Epoch 5, Loss: 1114.2545\n",
      "Best threshold: 0.444, MCC: 0.415, F1: 0.645\n",
      "Epoch 6, Loss: 1098.2159\n",
      "Best threshold: 0.455, MCC: 0.424, F1: 0.652\n",
      "Epoch 7, Loss: 1086.0694\n",
      "Best threshold: 0.384, MCC: 0.422, F1: 0.668\n",
      "Epoch 8, Loss: 1074.5797\n",
      "Best threshold: 0.455, MCC: 0.429, F1: 0.649\n",
      "Epoch 9, Loss: 1065.0141\n",
      "Best threshold: 0.434, MCC: 0.430, F1: 0.653\n",
      "Epoch 10, Loss: 1056.6342\n",
      "Best threshold: 0.455, MCC: 0.434, F1: 0.661\n",
      "Epoch 11, Loss: 1047.5919\n",
      "Best threshold: 0.465, MCC: 0.442, F1: 0.662\n",
      "Epoch 12, Loss: 1039.9255\n",
      "Best threshold: 0.424, MCC: 0.449, F1: 0.675\n",
      "Epoch 13, Loss: 1033.7687\n",
      "Best threshold: 0.455, MCC: 0.444, F1: 0.659\n",
      "Epoch 14, Loss: 1028.6712\n",
      "Best threshold: 0.475, MCC: 0.446, F1: 0.657\n",
      "Epoch 15, Loss: 1022.9614\n",
      "Best threshold: 0.495, MCC: 0.455, F1: 0.651\n",
      "Epoch 16, Loss: 1015.4217\n",
      "Best threshold: 0.444, MCC: 0.451, F1: 0.672\n",
      "Epoch 17, Loss: 1010.6820\n",
      "Best threshold: 0.465, MCC: 0.454, F1: 0.664\n",
      "Epoch 18, Loss: 1006.4270\n",
      "Best threshold: 0.505, MCC: 0.457, F1: 0.662\n",
      "Epoch 19, Loss: 1000.8299\n",
      "Best threshold: 0.495, MCC: 0.464, F1: 0.667\n",
      "Epoch 20, Loss: 995.1876\n",
      "Best threshold: 0.485, MCC: 0.454, F1: 0.655\n",
      "Epoch 21, Loss: 990.7867\n",
      "Best threshold: 0.525, MCC: 0.449, F1: 0.648\n",
      "Epoch 22, Loss: 989.2303\n",
      "Best threshold: 0.475, MCC: 0.456, F1: 0.666\n",
      "Epoch 23, Loss: 982.9418\n",
      "Best threshold: 0.475, MCC: 0.464, F1: 0.669\n",
      "Epoch 24, Loss: 979.8962\n",
      "Best threshold: 0.444, MCC: 0.455, F1: 0.667\n",
      "Epoch 25, Loss: 976.2584\n",
      "Best threshold: 0.434, MCC: 0.459, F1: 0.679\n",
      "Epoch 26, Loss: 971.0357\n",
      "Best threshold: 0.434, MCC: 0.457, F1: 0.669\n",
      "Epoch 27, Loss: 968.7422\n",
      "Best threshold: 0.424, MCC: 0.463, F1: 0.683\n",
      "Epoch 28, Loss: 962.7403\n",
      "Best threshold: 0.525, MCC: 0.461, F1: 0.654\n",
      "Epoch 29, Loss: 960.7538\n",
      "Best threshold: 0.444, MCC: 0.459, F1: 0.679\n",
      "Epoch 30, Loss: 957.9934\n",
      "Best threshold: 0.515, MCC: 0.459, F1: 0.658\n",
      "Epoch 31, Loss: 955.0973\n",
      "Best threshold: 0.414, MCC: 0.465, F1: 0.685\n",
      "Epoch 32, Loss: 951.8846\n",
      "Best threshold: 0.434, MCC: 0.461, F1: 0.677\n",
      "Epoch 33, Loss: 948.0468\n",
      "Best threshold: 0.495, MCC: 0.464, F1: 0.664\n",
      "Epoch 34, Loss: 945.1843\n",
      "Best threshold: 0.424, MCC: 0.466, F1: 0.683\n",
      "Epoch 35, Loss: 943.4276\n",
      "Best threshold: 0.475, MCC: 0.463, F1: 0.674\n",
      "Epoch 36, Loss: 939.7871\n",
      "Best threshold: 0.505, MCC: 0.464, F1: 0.666\n",
      "Epoch 37, Loss: 939.2049\n",
      "Best threshold: 0.444, MCC: 0.465, F1: 0.682\n",
      "Epoch 38, Loss: 934.8278\n",
      "Best threshold: 0.485, MCC: 0.463, F1: 0.663\n",
      "Epoch 39, Loss: 931.7687\n",
      "Best threshold: 0.465, MCC: 0.473, F1: 0.676\n",
      "Epoch 40, Loss: 929.2876\n",
      "Best threshold: 0.444, MCC: 0.457, F1: 0.671\n",
      "Epoch 41, Loss: 926.7670\n",
      "Best threshold: 0.475, MCC: 0.470, F1: 0.678\n",
      "Epoch 42, Loss: 924.1380\n",
      "Best threshold: 0.505, MCC: 0.465, F1: 0.665\n",
      "Epoch 43, Loss: 921.3675\n",
      "Best threshold: 0.444, MCC: 0.475, F1: 0.682\n",
      "Epoch 44, Loss: 921.3218\n",
      "Best threshold: 0.505, MCC: 0.463, F1: 0.666\n",
      "Epoch 45, Loss: 916.1290\n",
      "Best threshold: 0.495, MCC: 0.462, F1: 0.664\n",
      "Epoch 46, Loss: 914.7528\n",
      "Best threshold: 0.424, MCC: 0.460, F1: 0.682\n",
      "Epoch 47, Loss: 913.0349\n",
      "Best threshold: 0.535, MCC: 0.470, F1: 0.664\n",
      "Epoch 48, Loss: 909.8721\n",
      "Best threshold: 0.414, MCC: 0.460, F1: 0.682\n",
      "Epoch 49, Loss: 908.7584\n",
      "Best threshold: 0.485, MCC: 0.466, F1: 0.674\n",
      "Epoch 50, Loss: 905.7265\n",
      "Best threshold: 0.444, MCC: 0.463, F1: 0.677\n"
     ]
    }
   ],
   "source": [
    "# 2. Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=28)\n",
    "\n",
    "# 3. Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# 4. Define the model\n",
    "class SpeciesNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = SpeciesNet(X_train.shape[1]).to(device)\n",
    "criterion = nn.BCELoss().to(device)\n",
    "auroc = BinaryAUROC().to(device)\n",
    "auprc = BinaryAUPRC().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 5. Training loop\n",
    "train_auroc = []\n",
    "train_auprc = []\n",
    "val_auroc = []\n",
    "val_auprc = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        auroc.update(preds.squeeze(1), yb.squeeze(1))\n",
    "        auprc.update(preds.squeeze(1), yb.squeeze(1))\n",
    "    train_auroc.append(auroc.compute().cpu().numpy())\n",
    "    train_auprc.append(auprc.compute().cpu().numpy())\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    auroc.reset()\n",
    "    auprc.reset()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_probs = model(X_train_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    cal = IsotonicRegression(out_of_bounds='clip')\n",
    "    cal.fit(train_probs, y_train)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            val_probs_raw = model(X_val_tensor)\n",
    "    #         auroc.update(val_probs_raw.squeeze(1), yb.squeeze(1))\n",
    "    #         auprc.update(val_probs_raw.squeeze(1), yb.squeeze(1))\n",
    "    # val_auroc.append(auroc.compute().cpu().numpy())\n",
    "    # val_auprc.append(auprc.compute().cpu().numpy())\n",
    "\n",
    "    val_probs_cal = cal.predict(val_probs_raw.cpu().numpy().flatten())\n",
    "\n",
    "    # 7. Threshold tuning\n",
    "    best_mcc, best_f1, best_thresh = -1, -1, 0\n",
    "    for t in np.linspace(0, 1, 100):\n",
    "        preds = (val_probs_cal > t).astype(int)\n",
    "        m = matthews_corrcoef(y_val, preds)\n",
    "        f = f1_score(y_val, preds)\n",
    "        if m > best_mcc:\n",
    "            best_mcc, best_f1, best_thresh = m, f, t\n",
    "\n",
    "    print(f\"Best threshold: {best_thresh:.3f}, MCC: {best_mcc:.3f}, F1: {best_f1:.3f}\")\n",
    "    auroc.reset()\n",
    "    auprc.reset()\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2300d740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.444, MCC: 0.463, F1: 0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alonz\\AppData\\Local\\Temp\\ipykernel_19764\\689761987.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  grid_output[\"in_range\"] = (grid_output[\"encounter_rate\"] > best_thresh).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# 6. Predict and calibrate with isotonic regression\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_probs = model(X_train_tensor).cpu().numpy().flatten()\n",
    "\n",
    "cal = IsotonicRegression(out_of_bounds='clip')\n",
    "cal.fit(train_probs, y_train)\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_probs_raw = model(X_val_tensor).cpu().numpy().flatten()\n",
    "val_probs_cal = cal.predict(val_probs_raw)\n",
    "\n",
    "# 7. Threshold tuning\n",
    "best_mcc, best_f1, best_thresh = -1, -1, 0\n",
    "for t in np.linspace(0, 1, 100):\n",
    "    preds = (val_probs_cal > t).astype(int)\n",
    "    m = matthews_corrcoef(y_val, preds)\n",
    "    f = f1_score(y_val, preds)\n",
    "    if m > best_mcc:\n",
    "        best_mcc, best_f1, best_thresh = m, f, t\n",
    "\n",
    "print(f\"Best threshold: {best_thresh:.3f}, MCC: {best_mcc:.3f}, F1: {best_f1:.3f}\")\n",
    "\n",
    "## Generate Confusion matrix using tuned threshold and calibrated probabilities\n",
    "best_pred = (val_probs_cal > best_thresh).astype(int)\n",
    "confmat = confusion_matrix(y_val, best_pred, normalize = 'pred')\n",
    "\n",
    "# 8. Predict on grid\n",
    "grid = pd.read_csv(data_folder + \"environmental_vars_prediction_grid_md.csv\")\n",
    "grid[\"observation_date\"] = pd.to_datetime(\"2023-01-15\")\n",
    "grid[\"year\"] = grid[\"observation_date\"].dt.year\n",
    "grid[\"day_of_year\"] = grid[\"observation_date\"].dt.dayofyear\n",
    "grid[\"hours_of_day\"] = 7.5\n",
    "grid[\"effort_distance_km\"] = 2\n",
    "grid[\"effort_hours\"] = 1\n",
    "grid[\"effort_speed_kmph\"] = 2\n",
    "grid[\"number_observers\"] = 1\n",
    "\n",
    "X_grid = grid[features]\n",
    "X_grid_scaled = scaler.transform(X_grid)\n",
    "X_grid_tensor = torch.tensor(X_grid_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    grid_probs_raw = model(X_grid_tensor).cpu().numpy().flatten()\n",
    "grid_probs_cal = cal.predict(grid_probs_raw)\n",
    "grid[\"encounter_rate\"] = np.clip(grid_probs_cal, 0, 1)\n",
    "\n",
    "# Save outputs\n",
    "grid_output = grid[[\"cell_id\", \"x\", \"y\", \"encounter_rate\"]]\n",
    "grid_output[\"in_range\"] = (grid_output[\"encounter_rate\"] > best_thresh).astype(int)\n",
    "grid_output.to_csv(f\"junco_nn_predictions_{num_epochs}.csv\", index=False)\n",
    "\n",
    "# Save validation predictions for R\n",
    "results_df = pd.DataFrame({\n",
    "    'obs': y_val,\n",
    "    'pred': val_probs_cal\n",
    "})\n",
    "results_df.to_csv(f\"dnn_predictions_for_r_{num_epochs}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cde5b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Grid Predictions')\n",
    "plt.scatter(grid_output['x'], grid_output['y'], c = grid_output['encounter_rate'].values, s = 10, marker = 's', alpha = 0.8)\n",
    "plt.colorbar()\n",
    "# plt.show()\n",
    "plt.savefig(f'dnn_grid_pred_{num_epochs}.png')\n",
    "plt.close()\n",
    "sns.heatmap(confmat, annot=True, cmap='Greens')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "plt.savefig(f'dnn_conf_mat_{num_epochs}.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2332d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot auroc and auprc\n",
    "plt.plot(range(num_epochs), train_auroc)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUROC')\n",
    "plt.title('Training AUROC')\n",
    "# plt.show()\n",
    "plt.savefig(f'dnn_train_auroc_{num_epochs}.png')\n",
    "plt.close()\n",
    "plt.plot(range(num_epochs), train_auprc)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUPRC Score')\n",
    "plt.title('Training AUPRC Score')\n",
    "# plt.show()\n",
    "plt.savefig(f'dnn_train_auprc_{num_epochs}.png')\n",
    "plt.close()\n",
    "# plt.plot(range(num_epochs), val_auroc)\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('AUROC')\n",
    "# plt.title('Validation AUROC')\n",
    "# plt.savefig(f'dnn_val_auroc_{num_epochs}.png')\n",
    "# plt.close()\n",
    "# plt.plot(range(num_epochs), val_auprc)\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('AUPRC Score')\n",
    "# plt.title('Validation AUPRC Score')\n",
    "# plt.savefig(f'dnn_val_auprc_{num_epochs}.png')\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81155ad",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d9b83e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1246.5670\n",
      "Best threshold: 0.444, MCC: 0.358, auprc: 0.601\n",
      "Epoch 2, Loss: 1171.6347\n",
      "Best threshold: 0.434, MCC: 0.383, auprc: 0.635\n",
      "Epoch 3, Loss: 1137.3757\n",
      "Best threshold: 0.394, MCC: 0.403, auprc: 0.662\n",
      "Epoch 4, Loss: 1112.2611\n",
      "Best threshold: 0.485, MCC: 0.418, auprc: 0.623\n",
      "Epoch 5, Loss: 1089.5718\n",
      "Best threshold: 0.444, MCC: 0.432, auprc: 0.663\n",
      "Epoch 6, Loss: 1071.8151\n",
      "Best threshold: 0.434, MCC: 0.438, auprc: 0.658\n",
      "Epoch 7, Loss: 1056.3800\n",
      "Best threshold: 0.414, MCC: 0.455, auprc: 0.679\n",
      "Epoch 8, Loss: 1042.2478\n",
      "Best threshold: 0.465, MCC: 0.438, auprc: 0.647\n",
      "Epoch 9, Loss: 1029.7138\n",
      "Best threshold: 0.434, MCC: 0.453, auprc: 0.673\n",
      "Epoch 10, Loss: 1017.2783\n",
      "Best threshold: 0.434, MCC: 0.458, auprc: 0.678\n",
      "Epoch 11, Loss: 1002.5101\n",
      "Best threshold: 0.495, MCC: 0.457, auprc: 0.656\n",
      "Epoch 12, Loss: 991.9538\n",
      "Best threshold: 0.444, MCC: 0.463, auprc: 0.674\n",
      "Epoch 13, Loss: 980.7090\n",
      "Best threshold: 0.455, MCC: 0.464, auprc: 0.672\n",
      "Epoch 14, Loss: 969.9034\n",
      "Best threshold: 0.525, MCC: 0.457, auprc: 0.648\n",
      "Epoch 15, Loss: 959.4755\n",
      "Best threshold: 0.505, MCC: 0.459, auprc: 0.658\n",
      "Epoch 16, Loss: 952.0511\n",
      "Best threshold: 0.434, MCC: 0.464, auprc: 0.678\n",
      "Epoch 17, Loss: 940.4312\n",
      "Best threshold: 0.485, MCC: 0.465, auprc: 0.668\n",
      "Epoch 18, Loss: 932.9586\n",
      "Best threshold: 0.475, MCC: 0.463, auprc: 0.670\n",
      "Epoch 19, Loss: 923.7114\n",
      "Best threshold: 0.455, MCC: 0.463, auprc: 0.672\n",
      "Epoch 20, Loss: 914.6083\n",
      "Best threshold: 0.556, MCC: 0.464, auprc: 0.650\n",
      "Epoch 21, Loss: 907.5295\n",
      "Best threshold: 0.394, MCC: 0.463, auprc: 0.688\n",
      "Epoch 22, Loss: 899.3087\n",
      "Best threshold: 0.465, MCC: 0.467, auprc: 0.680\n",
      "Epoch 23, Loss: 890.8443\n",
      "Best threshold: 0.556, MCC: 0.465, auprc: 0.648\n",
      "Epoch 24, Loss: 881.5463\n",
      "Best threshold: 0.586, MCC: 0.461, auprc: 0.644\n",
      "Epoch 25, Loss: 874.8194\n",
      "Best threshold: 0.455, MCC: 0.468, auprc: 0.682\n",
      "Epoch 26, Loss: 866.3474\n",
      "Best threshold: 0.414, MCC: 0.466, auprc: 0.686\n",
      "Epoch 27, Loss: 857.7429\n",
      "Best threshold: 0.475, MCC: 0.466, auprc: 0.670\n",
      "Epoch 28, Loss: 849.7449\n",
      "Best threshold: 0.465, MCC: 0.459, auprc: 0.670\n",
      "Epoch 29, Loss: 840.8749\n",
      "Best threshold: 0.424, MCC: 0.463, auprc: 0.683\n",
      "Epoch 30, Loss: 833.1828\n",
      "Best threshold: 0.444, MCC: 0.471, auprc: 0.686\n",
      "Epoch 31, Loss: 827.6404\n",
      "Best threshold: 0.485, MCC: 0.459, auprc: 0.666\n",
      "Epoch 32, Loss: 819.9790\n",
      "Best threshold: 0.465, MCC: 0.463, auprc: 0.673\n",
      "Epoch 33, Loss: 812.8048\n",
      "Best threshold: 0.364, MCC: 0.460, auprc: 0.690\n",
      "Epoch 34, Loss: 803.7412\n",
      "Best threshold: 0.495, MCC: 0.460, auprc: 0.662\n",
      "Epoch 35, Loss: 800.2047\n",
      "Best threshold: 0.444, MCC: 0.459, auprc: 0.680\n",
      "Epoch 36, Loss: 792.9509\n",
      "Best threshold: 0.404, MCC: 0.455, auprc: 0.680\n",
      "Epoch 37, Loss: 785.5707\n",
      "Best threshold: 0.586, MCC: 0.456, auprc: 0.654\n",
      "Epoch 38, Loss: 783.9740\n",
      "Best threshold: 0.354, MCC: 0.456, auprc: 0.686\n",
      "Epoch 39, Loss: 772.3667\n",
      "Best threshold: 0.404, MCC: 0.452, auprc: 0.681\n",
      "Epoch 40, Loss: 766.4485\n",
      "Best threshold: 0.505, MCC: 0.446, auprc: 0.662\n",
      "Epoch 41, Loss: 761.3622\n",
      "Best threshold: 0.525, MCC: 0.457, auprc: 0.665\n",
      "Epoch 42, Loss: 757.0198\n",
      "Best threshold: 0.455, MCC: 0.453, auprc: 0.673\n",
      "Epoch 43, Loss: 747.7080\n",
      "Best threshold: 0.576, MCC: 0.445, auprc: 0.645\n",
      "Epoch 44, Loss: 743.0548\n",
      "Best threshold: 0.434, MCC: 0.441, auprc: 0.671\n",
      "Epoch 45, Loss: 738.0261\n",
      "Best threshold: 0.444, MCC: 0.448, auprc: 0.671\n",
      "Epoch 46, Loss: 731.3256\n",
      "Best threshold: 0.444, MCC: 0.443, auprc: 0.666\n",
      "Epoch 47, Loss: 726.7196\n",
      "Best threshold: 0.384, MCC: 0.440, auprc: 0.675\n",
      "Epoch 48, Loss: 717.3999\n",
      "Best threshold: 0.424, MCC: 0.449, auprc: 0.673\n",
      "Epoch 49, Loss: 714.4010\n",
      "Best threshold: 0.515, MCC: 0.444, auprc: 0.662\n",
      "Epoch 50, Loss: 704.9872\n",
      "Best threshold: 0.465, MCC: 0.445, auprc: 0.665\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class SpeciesNet1DCNN(nn.Module):\n",
    "    def __init__(self, input_length: int):\n",
    "        super().__init__()\n",
    "        # 1) 1D-CNN feature extractor\n",
    "        self.cnn = nn.Sequential(\n",
    "            # in_channels=1, out_channels=16, length stays 39 (padding=1)\n",
    "            nn.Conv1d(1, 16, kernel_size=3, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),            # -> length = floor(39/2) = 19\n",
    "\n",
    "            nn.Conv1d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),            # -> length = floor(19/2) = 9\n",
    "        )\n",
    "        # compute flattened size after two pools\n",
    "        conv_out_len = input_length // 2 // 2    # 39→19→9\n",
    "        flattened_dim = 32 * conv_out_len        # 32 channels × length 9 = 288\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flattened_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, 1, 39)\n",
    "        x = self.cnn(x)                 # -> (batch, 32, 9)\n",
    "        x = x.view(x.size(0), -1)       # -> (batch, 32*9)\n",
    "        return self.fc(x)               # -> (batch, 1)\n",
    "\n",
    "# ——— data prep ———\n",
    "# 2. Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=28)\n",
    "# add a channel dimension of size 1:\n",
    "X_train_1d = X_train.reshape(-1, 1, 39)  \n",
    "X_val_1d   = X_val.reshape(  -1, 1, 39)\n",
    "\n",
    "# 3. Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_1d, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val_1d, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "train_ds = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_ds   = TensorDataset(X_val_tensor,   y_val_tensor)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32)\n",
    "\n",
    "# ——— model, loss, optimizer ———\n",
    "model = SpeciesNet1DCNN(input_length=39).to(device)\n",
    "criterion = nn.BCELoss().to(device)\n",
    "auroc = BinaryAUROC().to(device)\n",
    "auprc = BinaryAUPRC().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 5. Training loop\n",
    "train_auroc = []\n",
    "train_auprc = []\n",
    "val_auroc = []\n",
    "val_auprc = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        auroc.update(preds.squeeze(1), yb.squeeze(1))\n",
    "        auprc.update(preds.squeeze(1), yb.squeeze(1))\n",
    "    train_auroc.append(auroc.compute().cpu().numpy())\n",
    "    train_auprc.append(auprc.compute().cpu().numpy())\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    auroc.reset()\n",
    "    auprc.reset()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_probs = model(X_train_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    cal = IsotonicRegression(out_of_bounds='clip')\n",
    "    cal.fit(train_probs, y_train)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            val_probs_raw = model(X_val_tensor)\n",
    "    #         auroc.update(val_probs_raw.squeeze(1), yb.squeeze(1))\n",
    "    #         auprc.update(val_probs_raw.squeeze(1), yb.squeeze(1))\n",
    "    # val_auroc.append(auroc.compute().cpu().numpy())\n",
    "    # val_auprc.append(auprc.compute().cpu().numpy())\n",
    "\n",
    "    val_probs_cal = cal.predict(val_probs_raw.cpu().numpy().flatten())\n",
    "\n",
    "    # 7. Threshold tuning\n",
    "    best_mcc, best_auprc, best_thresh = -1, -1, 0\n",
    "    for t in np.linspace(0, 1, 100):\n",
    "        preds = (val_probs_cal > t).astype(int)\n",
    "        m = matthews_corrcoef(y_val, preds)\n",
    "        f = f1_score(y_val, preds)\n",
    "        if m > best_mcc:\n",
    "            best_mcc, best_f1, best_thresh = m, f, t\n",
    "\n",
    "    print(f\"Best threshold: {best_thresh:.3f}, MCC: {best_mcc:.3f}, auprc: {best_f1:.3f}\")\n",
    "    auroc.reset()\n",
    "    auprc.reset()\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d035750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.470, MCC: 0.445, F1: 0.665\n"
     ]
    }
   ],
   "source": [
    "# 6. Predict and calibrate with isotonic regression\n",
    "model.eval()\n",
    "\n",
    "# ——— get train probs ———\n",
    "with torch.no_grad():\n",
    "    train_probs = model(X_train_tensor).cpu().numpy().flatten()\n",
    "\n",
    "cal = IsotonicRegression(out_of_bounds='clip')\n",
    "cal.fit(train_probs, y_train)\n",
    "\n",
    "# ——— get val probs ———\n",
    "with torch.no_grad():\n",
    "    val_probs_raw = model(X_val_tensor).cpu().numpy().flatten()\n",
    "val_probs_cal = cal.predict(val_probs_raw)\n",
    "\n",
    "# 7. Threshold tuning on validation set\n",
    "best_mcc, best_f1, best_thresh = -1, -1, 0.0\n",
    "for t in np.linspace(0, 1, 101):\n",
    "    preds_t = (val_probs_cal > t).astype(int)\n",
    "    m = matthews_corrcoef(y_val, preds_t)\n",
    "    f = f1_score(y_val, preds_t)\n",
    "    if m > best_mcc:\n",
    "        best_mcc, best_f1, best_thresh = m, f, t\n",
    "\n",
    "print(f\"Best threshold: {best_thresh:.3f}, MCC: {best_mcc:.3f}, F1: {best_f1:.3f}\")\n",
    "\n",
    "## Generate Confusion matrix using tuned threshold and calibrated probabilities\n",
    "best_pred = (val_probs_cal > best_thresh).astype(int)\n",
    "confmat = confusion_matrix(y_val, best_pred, normalize = 'pred')\n",
    "\n",
    "# 8. Predict on grid\n",
    "grid = pd.read_csv(data_folder + \"environmental_vars_prediction_grid_md.csv\")\n",
    "grid[\"observation_date\"] = pd.to_datetime(\"2023-01-15\")\n",
    "grid[\"year\"]           = grid[\"observation_date\"].dt.year\n",
    "grid[\"day_of_year\"]    = grid[\"observation_date\"].dt.dayofyear\n",
    "grid[\"hours_of_day\"]   = 7.5\n",
    "grid[\"effort_distance_km\"] = 2\n",
    "grid[\"effort_hours\"]       = 1\n",
    "grid[\"effort_speed_kmph\"]  = 2\n",
    "grid[\"number_observers\"]   = 1\n",
    "\n",
    "# select & scale features\n",
    "X_grid = grid[features]\n",
    "X_grid_scaled = scaler.transform(X_grid)\n",
    "\n",
    "# reshape for 1D-CNN: (N, 39) -> (N, 1, 39)\n",
    "X_grid_1d = X_grid_scaled.reshape(-1, 1, X_grid_scaled.shape[1])\n",
    "X_grid_tensor = torch.tensor(X_grid_1d, dtype=torch.float32).to(device)\n",
    "\n",
    "# Batch grid code:\n",
    "# grid_ds    = TensorDataset(X_grid_tensor)\n",
    "# grid_loader= DataLoader(grid_ds, batch_size=1024, shuffle=False)\n",
    "# grid_probs_raw = []\n",
    "# with torch.no_grad():\n",
    "#     for (xb,) in grid_loader:\n",
    "#         grid_probs_raw.append(model(xb).cpu().numpy().flatten())\n",
    "# grid_probs_raw = np.concatenate(grid_probs_raw)\n",
    "\n",
    "with torch.no_grad():\n",
    "    grid_probs_raw = model(X_grid_tensor).cpu().numpy().flatten()\n",
    "\n",
    "# calibrate & clip\n",
    "grid_probs_cal = cal.predict(grid_probs_raw)\n",
    "grid[\"encounter_rate\"] = np.clip(grid_probs_cal, 0, 1)\n",
    "\n",
    "# assemble & save\n",
    "grid_output = grid[[\"cell_id\", \"x\", \"y\", \"encounter_rate\"]].copy()\n",
    "grid_output[\"in_range\"] = (grid_output[\"encounter_rate\"] > best_thresh).astype(int)\n",
    "grid_output.to_csv(f\"junco_cnn_predictions_{num_epochs}.csv\", index=False)\n",
    "\n",
    "# also save validation predictions for R\n",
    "results_df = pd.DataFrame({\n",
    "    'obs':  y_val,\n",
    "    'pred': val_probs_cal\n",
    "})\n",
    "results_df.to_csv(f\"cnn_predictions_for_r_{num_epochs}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "87ee7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Grid Predictions')\n",
    "plt.scatter(grid_output['x'], grid_output['y'], c = grid_output['encounter_rate'].values, s = 10, marker = 's', alpha = 0.8)\n",
    "plt.colorbar()\n",
    "# plt.show()\n",
    "plt.savefig(f'cnn_grid_pred_{num_epochs}.png')\n",
    "plt.close()\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(confmat, annot=True, cmap='Greens')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "plt.savefig(f'cnn_conf_mat_{num_epochs}.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a3936394",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot AUROC and f1\n",
    "plt.plot(range(num_epochs), train_auroc)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUROC')\n",
    "plt.title('Training AUROC')\n",
    "# plt.show()\n",
    "plt.savefig(f'cnn_train_auroc_{num_epochs}.png')\n",
    "plt.close()\n",
    "plt.plot(range(num_epochs), train_auprc)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUPRC')\n",
    "plt.title('Training AUPRC')\n",
    "# plt.show()\n",
    "plt.savefig(f'cnn_train_auprc_{num_epochs}.png')\n",
    "plt.close()\n",
    "# plt.plot(range(num_epochs), val_auroc)\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('AUROC')\n",
    "# plt.title('Validation AUROC')\n",
    "# plt.savefig(f'cnn_val_auroc_{num_epochs}.png')\n",
    "# plt.close()\n",
    "# plt.plot(range(num_epochs), val_auprc)\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('AUPRC')\n",
    "# plt.title('Validation AUPRC')\n",
    "# plt.savefig(f'cnn_val_auprc_{num_epochs}.png')\n",
    "# plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
